{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f15f870",
   "metadata": {},
   "source": [
    "# Hackathon Enedis\n",
    "\n",
    "Code et Notebook réalisés par : **Tom Le Ber**.\n",
    "\n",
    "Site web du hackthon : [https://challengedata.ens.fr/participants/challenges/160/](https://challengedata.ens.fr/participants/challenges/160/).\n",
    "\n",
    "*Vous pouvez également retrouver tout le code écrit dans ce notebook dans les fichiers `solution.py` et `utils.py` de ce même dépot GitHub.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2dace",
   "metadata": {},
   "source": [
    "## Imports des bibliothèques nécessaires pour la bonne exécution de ce notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib scikit-learn joblib tqdm optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm # Sert juste à avoir une représentation visuelle de la progression des boucles\n",
    "import optuna\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1deaa7c",
   "metadata": {},
   "source": [
    "## I. Explication de la méthode de résolution : Imputation par régression avec hyperparamètres optimisés\n",
    "\n",
    "### A. Régression Ridge\n",
    "\n",
    "Ridge est un modèle de **régression linéaire** avec une pénalisation $L2$ sur les coefficients.\n",
    "\n",
    "On cherche un vecteur de paramètres $a \\in \\mathbb{R}^d$ et un biais $b \\in \\mathbb{R}$ qui minimisent :\n",
    "\n",
    "$$\n",
    "  \\mathcal{L}_{\\text{ridge}}(a, b) = \\frac{1}{n} \\sum_{i=1}^n \\big( y_i - (a x_i + b) \\big)^2 \\;+\\; \\alpha \\, \\lVert a \\rVert_2^2\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- Le premier terme est l'erreur quadratique moyenne ($MSE$),\n",
    "- Le second terme est la pénalisation $L2$,\n",
    "- $\\alpha > 0$ contrôle la force de la régularisation.\n",
    "\n",
    "Intuitivement, Ridge :\n",
    "- Évite que les coefficients deviennent trop grands,\n",
    "- Améliore la stabilité du modèle quand les features sont corrélées ou bruitées,\n",
    "- Réduit la variance au prix d'un peu plus de biais.\n",
    "\n",
    "Il est important de mentionner que Ridge est sensible à l'échelle des variables, une feature en `[0, 1]` et une autre en `[0, 10^6]` seraient pénalisées de façon très différente.\n",
    "\n",
    "On utilise donc un `StandardScaler` qui effectue, feature par feature, un centrage réduction de moyenne $0$ et d'écart-type $1$ :\n",
    "$$\n",
    "  {x_j}_{\\text{scaled}} = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- Chaque feature correspond à une colonne du tableau de données,\n",
    "- $\\mu_j$ est la moyenne de la feature $j$,\n",
    "- $\\sigma_j$ est l'écart-type de la feature $j$.\n",
    "\n",
    "Voici un exemple comparant une régression linéaire classique et une régression Ridge avec `alpha=10` sur un jeu de données quasi-linéaire (légèrement bruité) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292adbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(100, 1)\n",
    "y = 3 * X[:, 0] + rng.randn(100) * 0.5\n",
    "\n",
    "models = {\n",
    "    \"Régression Linéaire\": make_pipeline(StandardScaler(), LinearRegression()),\n",
    "    \"Régression Ridge (alpha=10)\": make_pipeline(StandardScaler(), Ridge(alpha=10)),\n",
    "}\n",
    "\n",
    "xx = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(X[:, 0], y, alpha=0.5, color='gray')\n",
    "for name, model in models.items():\n",
    "    model.fit(X, y)\n",
    "    yy = model.predict(xx)\n",
    "    plt.plot(xx[:, 0], yy, label=name)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Exemple de régression linéaire vs Ridge\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710fb02",
   "metadata": {},
   "source": [
    "### B. Gradient Boosting sur arbres\n",
    "\n",
    "Le **Gradient Boosting** construit un modèle comme somme d'arbres de décision :\n",
    "$$\n",
    "  F_M(x) = \\sum_{m=1}^M \\gamma_m \\, h_m(x)\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- Chaque $h_m$ est un arbre de décision peu profond,\n",
    "- Chaque nouvel arbre est entraîné pour corriger les erreurs du modèle précédent,\n",
    "- On minimise une fonction de perte (ici typiquement la $MSE$) par descente de gradient fonctionnelle.\n",
    "\n",
    "Intuitivement :\n",
    "- Le premier arbre capture une tendance grossière,\n",
    "- Les suivants corrigent progressivement les résidus.\n",
    "\n",
    "La version `HistGradientBoostingRegressor` de `sklearn` est une implémentation optimisée :\n",
    "- Les features sont discrétisées en \"buckets\" (histogrammes),\n",
    "- Cela accélère énormément la recherche des meilleurs splits,\n",
    "- Bien adapté aux gros tableaux de données numériques.\n",
    "\n",
    "Contrairement à Ridge, les arbres ne nécessitent **pas** de standardisation des features, ils capturent naturellement des non-linéarités (seuils, interactions, effets locaux, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7321bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "y_true = np.sin(X[:, 0])  # vraie fonction\n",
    "y = y_true + rng.normal(scale=0.3, size=X.shape[0])  # bruit\n",
    "\n",
    "# Modèle de Gradient Boosting :\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Choix de quelques étapes pour visualiser la construction progressive :\n",
    "stages = [0, 1, 5, 20, 50, 100]\n",
    "xx = np.linspace(-3, 3, 500).reshape(-1, 1)\n",
    "\n",
    "# On pré-calcule toutes les prédictions visualisées pour éviter de refaire la boucle à chaque fois :\n",
    "staged_preds = list(gbr.staged_predict(xx))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for index, nb_trees in enumerate(stages, start=1):\n",
    "    if nb_trees == 0:\n",
    "        # prédiction du modèle initial (avant tout arbre) :\n",
    "        y_pred = gbr.init_.predict(xx)\n",
    "        titre = \"0 arbres (modèle initial)\"\n",
    "        label_pred = \"Modèle initial (0 arbres)\"\n",
    "    else:\n",
    "        # prédiction après n_trees arbres\n",
    "        y_pred = staged_preds[nb_trees - 1]\n",
    "        titre = f\"{nb_trees} arbre{'s' if nb_trees != 1 else ''}\"\n",
    "        label_pred = f\"Après {nb_trees} arbre{'s' if nb_trees != 1 else ''}\"\n",
    "\n",
    "    plt.subplot(3, 2, index)\n",
    "    plt.scatter(X[:, 0], y, s=10, alpha=0.75, color='gray', label=\"Données bruitées\")\n",
    "    plt.plot(xx[:, 0], np.sin(xx[:, 0]), linewidth=2, label=\"Vraie fonction\")\n",
    "    plt.plot(xx[:, 0], y_pred, linewidth=2, label=label_pred)\n",
    "    plt.title(titre)\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle(\"Construction progressive du modèle par Gradient Boosting\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c8143",
   "metadata": {},
   "source": [
    "### C. Le modèle final : une combinaison de Ridge et HGBR\n",
    "\n",
    "Le modèle final, pour un vecteur de features $x$, combine deux prédictions :\n",
    "- $\\hat{y}_{\\text{ridge}}(x)$ : une prédiction du modèle `Ridge` (linéaire),\n",
    "- $\\hat{y}_{\\tiny{HGBR}}(x)$ : une prédiction du modèle `HistGradientBoostingRegressor` (non linéaire).\n",
    "\n",
    "La prédiction finale est une moyenne pondérée :\n",
    "$$\n",
    "  \\hat{y}(x) = w_{\\text{ridge}} \\, \\hat{y}_{\\text{ridge}}(x) + \\big(1 - w_{\\text{ridge}}\\big) \\, \\hat{y}_{\\tiny{HGBR}}(x)\n",
    "$$\n",
    "\n",
    "Dans mon code, ce poids est contrôlé par `RIDGE_PROPORTION`.\n",
    "\n",
    "Cotte combinaison mêle les aventages des deux modèles :\n",
    "- Ridge apporte une tendance globale, plus \"lisse\" et régulière,\n",
    "- HGBR apporte de la flexibilité pour capturer les non-linéarités,\n",
    "- La combinaison permet quasiment tout le temps d'avoir un meilleur résultat que celui obtenu par chaque modèle séparément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34d6d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_impute_ridge_HGBR(\n",
    "        x_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        x_missing: pd.DataFrame,\n",
    "        ridge_parameters: dict,\n",
    "        hgbr_parameters: dict,\n",
    "        ridge_proportion: float\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Entraîne les deux modèles donnés ci-dessous et retourne une prédiction combinée, pondérée selon RIDGE_PROPORTION.\n",
    "        1. Ridge (Linéaire), nécessite une standardisation des données.\n",
    "        2. HistGradientBoostingRegressor (Non-Linéaire).\n",
    "\n",
    "    Args:\n",
    "        x_train (pd.DataFrame): Données d'entraînement (voisins complètes).\n",
    "        y_train (pd.Series): Cible d'entraînement (valeurs complètes).\n",
    "        x_missing (pd.DataFrame): Données à prédire (voisins avec trous).\n",
    "        ridge_parameters (dict): Dictionnaire des hyperparamètres pour le modèle Ridge.\n",
    "        hgbr_parameters (dict): Dictionnaire des hyperparamètres pour le modèle HGBR.\n",
    "        ridge_proportion (float): Poids pour la prédiction Ridge dans l'ensemble.\n",
    "    Returns:\n",
    "        np.ndarray: Prédictions combinées pour les données manquantes.\n",
    "    \"\"\"\n",
    "    # Création des scalers pour la standardisation des données :\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_missing_scaled = scaler.transform(x_missing)\n",
    "\n",
    "    # Création et entraînement des modèles :\n",
    "        # 1. Ridge (Linéaire) :\n",
    "    ridge_model = Ridge(**ridge_parameters)\n",
    "    ridge_model.fit(x_train_scaled, y_train)\n",
    "    ridge_prediction = ridge_model.predict(x_missing_scaled)\n",
    "        # 2. Gradient Boosting (Non-Linéaire) :\n",
    "    HGBR_model = HistGradientBoostingRegressor(**hgbr_parameters)\n",
    "    HGBR_model.fit(x_train, y_train)\n",
    "    HGBR_prediction = HGBR_model.predict(x_missing)\n",
    "\n",
    "    # Combinaison des prédictions :\n",
    "    return (ridge_proportion * ridge_prediction) + ((1 - ridge_proportion) * HGBR_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9ccc0",
   "metadata": {},
   "source": [
    "*Et il ne nous reste plus \"qu'à\" déterminer le poids optimal `RIDGE_PROPORTION` par validation croisée...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea70187",
   "metadata": {},
   "source": [
    "### D. Stratégie d’imputation d’une colonne `holed_*`\n",
    "\n",
    "Pour prédire une colonne cible `holed_k` :\n",
    "1. On identifie les lignes où la valeur est présente / manquante.\n",
    "2. On sélectionne parmi `complete_columns` les colonnes prédictives candidates.\n",
    "3. On calcule la corrélation entre chaque feature candidate et la colonne cible (sur les lignes sans NaN).\n",
    "4. On garde les `NB_NEIGHBORS` colonnes les plus corrélées en valeur absolue.\n",
    "5. On entraîne l'ensemble `Ridge` + `HGBR` sur ces lignes complètes et ces colonnes.\n",
    "6. On prédit les valeurs manquantes et on les insère dans la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_column(\n",
    "        df: pd.DataFrame,\n",
    "        name_column_to_fill: str,\n",
    "        complete_columns: list,\n",
    "        nb_neighbors: int,\n",
    "        ridge_parameters: dict,\n",
    "        hgbr_parameters: dict,\n",
    "        ridge_proportion: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes dans une colonne cible donnée en utilisant les colonnes complètes spécifiées.\n",
    "    Utilise une combinaison de Ridge et HistGradientBoostingRegressor pour l'imputation.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant toutes les données.\n",
    "        name_column_to_fill (str): Nom de la colonne cible à imputer.\n",
    "        complete_columns (list): Liste des noms de colonnes complètes à utiliser comme prédicteurs.\n",
    "        nb_neighbors (int): Nombre de voisins les plus corrélés à utiliser pour l'imputation.\n",
    "        ridge_parameters (dict): Dictionnaire des hyperparamètres pour le modèle Ridge.\n",
    "        hgbr_parameters (dict): Dictionnaire des hyperparamètres pour le modèle HGBR.\n",
    "        ridge_proportion (float): Poids pour la prédiction Ridge dans l'ensemble.\n",
    "    Returns:\n",
    "        pd.Series: Série contenant la colonne imputée.\n",
    "    \"\"\"\n",
    "    # Suppression des warnings 'inutiles' dans la console :\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    # Extraction de la colonne cible et création des masques :\n",
    "    y_full_input = df[name_column_to_fill]\n",
    "\n",
    "    mask_missing = y_full_input.isna()\n",
    "    mask_valid = ~mask_missing\n",
    "\n",
    "    # Si la colonne est déjà complète, on la retourne telle quelle :\n",
    "    if not mask_missing.any():\n",
    "        return y_full_input\n",
    "\n",
    "    y_valid = y_full_input[mask_valid] # Les valeurs connues à imputer.\n",
    "\n",
    "    # Sélection des colonnes / features candidates \"voisines\" de la colonne cible :\n",
    "    x_candidates = df.loc[mask_valid, complete_columns]\n",
    "\n",
    "    # Calcul des corrélations entre les colonnes candidates et la colonne cible :\n",
    "    correlations = x_candidates.corrwith(y_valid)\n",
    "    correlations = correlations.dropna()\n",
    "\n",
    "    # Si aucune corrélation n'est trouvée, on remplit avec la moyenne des valeurs connues :\n",
    "    if correlations.empty:\n",
    "        fill_value = y_valid.mean()\n",
    "        y_completed = y_full_input.fillna(fill_value)\n",
    "\n",
    "    # Sinon, on procède à l'imputation en utilisant les nb_neighbors voisins les plus corrélés :\n",
    "    else:\n",
    "        # Sélection des nb_neighbors voisins les plus corrélés (en valeur absolue) :\n",
    "        top_neighbors = correlations.abs().nlargest(nb_neighbors).index\n",
    "\n",
    "        # Préparation des données d'entraînement et de prédiction :\n",
    "        x_train = df.loc[mask_valid, top_neighbors]\n",
    "        y_train = y_valid\n",
    "        x_missing = df.loc[mask_missing, top_neighbors]\n",
    "\n",
    "        # Prédiction des valeurs manquantes :\n",
    "        y_predicted = ensemble_impute_ridge_HGBR(x_train, y_train, x_missing, ridge_parameters, hgbr_parameters, ridge_proportion)\n",
    "\n",
    "        # Remplissage des valeurs manquantes dans la série finale :\n",
    "        y_completed = y_full_input.copy()\n",
    "        y_completed.loc[mask_missing] = y_predicted\n",
    "\n",
    "    return y_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49afdd",
   "metadata": {},
   "source": [
    "### E. Imputation de toutes les colonnes `holed_*`\n",
    "\n",
    "Pour imputer toutes les colonnes `holed_*`, il suffit de répéter la procédure précédente pour chaque colonne cible :\n",
    "- La fonction `impute_holed_columns` suivante applique `impute_column` à chaque colonne `holed_*`,\n",
    "- En faisant (éventuellement) tourner en parallèle l'imputation de chaque colonne via `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da084a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_holed_columns(\n",
    "        df: pd.DataFrame,\n",
    "        columns_to_fill: list[str],\n",
    "        complete_columns: list[str],\n",
    "        nb_neighbors: int,\n",
    "        ridge_parameters: dict,\n",
    "        hgbr_parameters: dict,\n",
    "        ridge_proportion: float,\n",
    "        with_parallelism: bool = False,\n",
    "        nb_jobs: int = -1\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute les colonnes spécifiées dans 'columns_to_fill' en utilisant les colonnes complètes données dans 'complete_columns'.\n",
    "    Utilise le parallélisme pour (très largement) accélérer le processus.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant toutes les données.\n",
    "        columns_to_fill (list): Liste des noms de colonnes à imputer.\n",
    "        complete_columns (list): Liste des noms de colonnes complètes à utiliser comme prédicteurs.\n",
    "        nb_neighbors (int): Nombre de voisins les plus corrélés à utiliser pour l'imputation.\n",
    "        ridge_parameters (dict): Dictionnaire des hyperparamètres pour le modèle Ridge.\n",
    "        hgbr_parameters (dict): Dictionnaire des hyperparamètres pour le modèle HGBR.\n",
    "        ridge_proportion (float): Poids pour la prédiction Ridge dans l'ensemble.\n",
    "        with_parallelism (bool): Si vrai, utilise joblib.Parallel pour exécuter les imputations en parallèle.\n",
    "        nb_jobs (int): Nombre de jobs parallèles à utiliser si with_parallelism est vrai.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contenant les colonnes imputées.\n",
    "    \"\"\"\n",
    "    # Création et exécution des tâches parallèles :\n",
    "    if with_parallelism:\n",
    "        results = Parallel(n_jobs=nb_jobs)(\n",
    "            delayed(impute_column)(df, column, complete_columns, nb_neighbors, ridge_parameters, hgbr_parameters, ridge_proportion)\n",
    "            for column in tqdm(columns_to_fill)\n",
    "        )\n",
    "    else :\n",
    "        results = [\n",
    "            impute_column(df, column, complete_columns, nb_neighbors, ridge_parameters, hgbr_parameters, ridge_proportion)\n",
    "            for column in tqdm(columns_to_fill)\n",
    "        ]\n",
    "\n",
    "    # Reconstruction du DataFrame final :\n",
    "        # Convertion du résultat en un dictionnaire de (column_name, series) :\n",
    "    result_dict: dict[str, pd.Series] = dict(zip(columns_to_fill, results))\n",
    "        # Concaténation de toutes les séries prédites :\n",
    "    result_df = pd.concat(result_dict, axis=1)\n",
    "        # On restaure l'index d'origine (Horodate) afin de pouvoir exporter le résuktats en un CSV valide :\n",
    "    result_df.index.name = 'Horodate'\n",
    "    result_df.reset_index(inplace=True)\n",
    "        # Reconstruction des colonnes, dans le bon ordre :\n",
    "    columns_to_keep = ['Horodate'] + [column for column in result_df.columns if 'holed_' in column]\n",
    "\n",
    "    return result_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9d1f4",
   "metadata": {},
   "source": [
    "## II. Déterminaison des hyperparamètres\n",
    "\n",
    "### A. Les différents hyperparamètres\n",
    "\n",
    "Comme nous l'avons vu, le modèle final dépend de plusieurs hyperparamètres :\n",
    "- `nb_neighbors` : nombre de colonnes voisines les plus corrélées,\n",
    "- `ridge_alpha` : régularisation de Ridge,\n",
    "- `hgbr_iter` : nombre d'itérations (arbres) du HGBR,\n",
    "- `hgbr_depth` : profondeur maximale des arbres du HGBR,\n",
    "- `hgbr_lr` : learning rate du HGBR,\n",
    "- `w_ridge` : poids de Ridge dans l'ensemble.\n",
    "\n",
    "Or, la performance du modèle dépend fortement de ces hyperparamètres, il est donc essentiel de les choisir judicieusement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890079c",
   "metadata": {},
   "source": [
    "### B. Optuna pour l'optimisation bayésienne des hyperparamètres\n",
    "\n",
    "Optuna est une bibliothèque d'optimisation d'hyperparamètres qui utilise des techniques bayésiennes pour explorer efficacement l'espace des hyperparamètres.  \n",
    "Elle construit un modèle probabiliste de la fonction objectif (ici, la performance du modèle en fonction des hyperparamètres) et utilise ce modèle pour sélectionner les hyperparamètres à tester.\n",
    "Optuna est une des méthodes les plus efficaces pour l'optimisation d'hyperparamètres, dans le cadre de notre projet.\n",
    "\n",
    "Il nous suffit donc d'implémenter une fonction objectif qui, pour un ensemble d'hyperparamètres donnés (celui ci-dessus) avec des intervalles définis, entraîne le modèle et renvoie la performance ($MAE$) sur un jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_optimization_target(df_x: pd.DataFrame, df_y: pd.DataFrame, name_column_to_fill: str, parameters: dict, complete_columns: list) -> float:\n",
    "    \"\"\"\n",
    "    Evalue une colonne cible spécifique en utilisant les hyperparamètres donnés.\n",
    "    Retourne le MAE sur les valeurs manquantes uniquement.\n",
    "\n",
    "    Args:\n",
    "        df_x (pd.DataFrame): DataFrame avec les colonnes trouées.\n",
    "        df_y (pd.DataFrame): DataFrame avec les colonnes complètes (vérité terrain).\n",
    "        name_column_to_fill (str): Nom de la colonne cible à évaluer.\n",
    "        parameters (dict): Dictionnaire des hyperparamètres à utiliser.\n",
    "        complete_columns (list): Liste des noms de colonnes complètes à utiliser comme prédicteurs.\n",
    "    Returns:\n",
    "        float: MAE calculée sur les positions initialement manquantes.\n",
    "    \"\"\"\n",
    "    # Suppression des warnings 'inutiles' dans la console :\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Extraction de la colonne cible et création des masques :\n",
    "    y_true_full = df_y[name_column_to_fill]\n",
    "    y_input_holed = df_x[name_column_to_fill]\n",
    "    \n",
    "    mask_missing = y_input_holed.isna()\n",
    "    mask_valid = ~mask_missing\n",
    "\n",
    "    # Si la colonne est déjà complète, on retourne 0.0 comme erreur :\n",
    "    if not mask_missing.any(): \n",
    "        return 0.0\n",
    "\n",
    "    y_valid = y_input_holed[mask_valid] # Les valeurs connues à imputer.\n",
    "\n",
    "    # Sélection des colonnes / features candidates \"voisines\" de la colonne cible :\n",
    "    X_candidates = df_x.loc[mask_valid, complete_columns]\n",
    "\n",
    "    # Calcul des corrélations entre les colonnes candidates et la colonne cible :\n",
    "    corrs = X_candidates.corrwith(y_valid).abs()\n",
    "    top_k = corrs.nlargest(parameters['nb_neighbors']).index\n",
    "\n",
    "    # Préparation des données d'entraînement et de prédiction :\n",
    "    x_train = df_x.loc[mask_valid, top_k]\n",
    "    y_train = y_valid\n",
    "    x_missing = df_x.loc[mask_missing, top_k]\n",
    "\n",
    "    # Calcul des prédictions avec l'ensemble Ridge + HGBR :\n",
    "    y_predicted = ensemble_impute_ridge_HGBR(\n",
    "        x_train, y_train, x_missing,\n",
    "        ridge_parameters = {\n",
    "            'alpha': parameters['ridge_alpha'],\n",
    "            'random_state': SEED\n",
    "        },\n",
    "        hgbr_parameters = {\n",
    "            'max_iter': parameters['hgbr_iter'],\n",
    "            'max_depth': parameters['hgbr_depth'],\n",
    "            'learning_rate': parameters['hgbr_lr'],\n",
    "            'early_stopping': True,\n",
    "            'random_state': SEED\n",
    "        },\n",
    "        ridge_proportion = parameters['w_ridge']\n",
    "    )\n",
    "\n",
    "    # Calcul du MAE sur les positions initialement manquantes :\n",
    "    y_true_missing = y_true_full[mask_missing]\n",
    "    return np.mean(np.abs(y_true_missing - y_predicted)) # Pas exactement la même chose que dans utils.get_metrics, mais BEAUCOUP plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimized_parameters(\n",
    "        nb_holed_targets_for_optimization: int,\n",
    "        nb_trials: int,\n",
    "        input_file: str = 'data/datasets/x_train.csv',\n",
    "        output_file: str = 'data/datasets/y_train_true.csv',\n",
    "        with_parallelism: bool = False,\n",
    "        nb_jobs: int = -1\n",
    "    ) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Utilise Optuna pour trouver les hyperparamètres optimaux pour l'imputation des colonnes trouées.\n",
    "    Évalue les performances sur un sous-ensemble de colonnes cibles pour accélérer le processus.\n",
    "\n",
    "    Args:\n",
    "        nb_holed_targets_for_optimization (int): Nombre de colonnes 'holed_*' à échantillonner pour l'optimisation des hyperparamètres (parmi les 1000 disponibles).\n",
    "        nb_trials (int): Nombre de trials pour l'optimisation des hyperparamètres avec Optuna.\n",
    "        input_file (str):\n",
    "            Chemin vers le fichier CSV d'entrée avec les colonnes trouées.\n",
    "            Par défaut 'data/datasets/x_train.csv' (x_train.csv sur le site web).\n",
    "        output_file (str):\n",
    "            Chemin vers le fichier CSV de vérité terrain avec les colonnes complètes.\n",
    "            Par défaut 'data/datasets/y_train_true.csv' (y_train.csv sur le site web).\n",
    "        with_parallelism (bool): Si vrai, utilise joblib.Parallel pour accélérer le processus d'évaluation.\n",
    "        nb_jobs (int): Nombre de jobs parallèles à utiliser si with_parallelism est vrai.\n",
    "    Returns:\n",
    "        dict[str, float]: Dictionnaire des meilleurs hyperparamètres trouvés.\n",
    "    \"\"\"\n",
    "    # Ouverture des fichiers de données (et préparation des DataFrames) :\n",
    "    print(f\"Chargement des données pour l'optimisation des hyperparamètres.\")\n",
    "\n",
    "    df_x = pd.read_csv(input_file)\n",
    "    df_y = pd.read_csv(output_file)\n",
    "\n",
    "    df_x.set_index('Horodate', inplace=True)\n",
    "    df_y.set_index('Horodate', inplace=True)\n",
    "\n",
    "    # Sélection des colonnes cibles et prédicteurs :\n",
    "    holed_to_predict = [\n",
    "        column for column in df_x.columns if 'holed_' in column\n",
    "    ]\n",
    "\n",
    "    # Extration des colonnes d'entrainement (complètes et non-constantes) :\n",
    "    potential_predictors = [column for column in df_x.columns if 'holed_' not in column]\n",
    "    stds = df_x[potential_predictors].std()\n",
    "    complete_columns = stds[stds > 1e-9].index.tolist() # On enlève les colonnes constantes car elles sont (quasiment) inutiles pour Ridge et HGBR.\n",
    "\n",
    "    # Choix d'un sous-ensemble aléatoire de nb_holed_targets_for_optimization colonnes cibles (parmis les 1000 'holed_*' pour l'optimisation (trop lent sinon) :\n",
    "    np.random.seed(SEED)\n",
    "    subset_targets = np.random.choice(holed_to_predict, size=nb_holed_targets_for_optimization, replace=False)\n",
    "\n",
    "    print(f\"\\nRecherche des hyperparamètres les plus optimisés ({nb_trials} trials) :\\n\\t- Nombre de colonnes 'holed_*' échantillonnées\\t = {len(subset_targets)}\\n\\t- Nombre de colonnes prédictrices\\t\\t = {len(complete_columns)}\\n\")\n",
    "\n",
    "    # Définition de la fonction 'objective' d'Optuna :\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        parameters = {\n",
    "            'nb_neighbors': trial.suggest_int('nb_neighbors', 0, 10000),\n",
    "            'ridge_alpha':  trial.suggest_float('ridge_alpha', 1.0, 500.0, log=True),\n",
    "            'hgbr_iter':    trial.suggest_int('hgbr_iter', 100, 500),\n",
    "            'hgbr_depth':   trial.suggest_int('hgbr_depth', 4, 15),\n",
    "            'hgbr_lr':      trial.suggest_float('hgbr_lr', 0.01, 0.3),\n",
    "            'w_ridge':      trial.suggest_float('w_ridge', 0.0, 1.0) \n",
    "        }\n",
    "\n",
    "        if with_parallelism:\n",
    "            scores = Parallel(n_jobs=nb_jobs)(\n",
    "                delayed(evaluate_optimization_target)(df_x, df_y, column, parameters, complete_columns)\n",
    "                for column in subset_targets\n",
    "            )\n",
    "        else:\n",
    "            scores = [\n",
    "                evaluate_optimization_target(df_x, df_y, column, parameters, complete_columns)\n",
    "                for column in subset_targets\n",
    "            ]\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    # Lancement de l'optimisation avec Optuna :\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=nb_trials)\n",
    "\n",
    "    # Résultat final :\n",
    "    print(\"\\nOptimisation complète, meilleurs hyperparamètres trouvés :\\n\" + f\"\\n\\t\".join([f'- {key} = {value}' for key, value in study.best_params.items()]))\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611e202",
   "metadata": {},
   "source": [
    "### C. Exemple d'exécution de l'optimisation des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4329ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimized_parameters(\n",
    "    nb_holed_targets_for_optimization = 10,\n",
    "    nb_trials = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799741bf",
   "metadata": {},
   "source": [
    "### D. Hyperparamètres optimisés\n",
    "\n",
    "Après exécution de la fonction ci-dessus pour $100$ `holed_*` sur $250$ essais, on obtient les hyperparamètres optimaux suivants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_NEIGHBORS = 10000 # Plus il y a de voisins, plus le modèle peut capturer des relations complexes.\n",
    "\n",
    "RIDGE_PROPORTION = 0.125\n",
    "\n",
    "RIDGE_PARAMETERS = {\n",
    "    'alpha': 10,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "HGBR_PARAMETERS = {\n",
    "    'max_iter': 250,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.2,\n",
    "    'early_stopping': True,\n",
    "    'random_state': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9ccbc",
   "metadata": {},
   "source": [
    "## III. Exécution finale\n",
    "\n",
    "### A. Fonctions auxilliaires\n",
    "\n",
    "On introduit les fonctions suivantes afin :\n",
    "- De pouvoir estimer la $MAE$ et le $R^2$ d'un modèle d'imputation entrainé sur les colonnes `holed_*` du dataset `x_train.csv` (dont on connaît les vraies valeurs avec `y_train.csv`),\n",
    "  - Ceci permet de juger de la qualité de notre imputation avant de l'appliquer sur le dataset de test.\n",
    "- Et de pouvoir lire les fichiers CSV donnés, en fixant la colonne `Horodate` comme index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40d0ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(holed_full: pd.DataFrame, holed_predicted: pd.DataFrame, holed_nans: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calcule la MAE et le R² pour un ensemble de colonnes trouées données, uniquement sur les positions contenant des trous (NaNs) à l'origine.\n",
    "    Ignore les positions où holed_full ou holed_predicted sont NaN.\n",
    "\n",
    "    Args:\n",
    "        holed_full (pd.DataFrame): DataFrame avec les vraies valeurs complètes.\n",
    "        holed_predicted (pd.DataFrame): DataFrame avec les valeurs prédites.\n",
    "        holed_nans (pd.DataFrame): DataFrame avec les trous (NaNs) d'origine.\n",
    "    Raises:\n",
    "        ValueError: Si les shapes des DataFrames ne correspondent pas.\n",
    "    Returns:\n",
    "        Tuple[float, float]: MAE et R² calculés sur les positions initialement manquantes.\n",
    "    \"\"\"\n",
    "    # On s'assure que les shapes sont compatibles :\n",
    "    if holed_full.shape != holed_predicted.shape or holed_full.shape != holed_nans.shape:\n",
    "        raise ValueError(\n",
    "            f\"Shapes incompatibles :\\n\\t- holed_full = {holed_full.shape}\\n\\t- holed_predicted = {holed_predicted.shape}\\n\\t- holed_nans = {holed_nans.shape}\"\n",
    "        )\n",
    "\n",
    "    # On masque des trous d'origine :\n",
    "    mask_missing_values = holed_nans.isna().values # Renvoie un tableau 2D de booléens.\n",
    "\n",
    "    # On récupère les valeurs réelles et prédites :\n",
    "    true_holed = holed_full.values\n",
    "    predicted_holed = holed_predicted.values\n",
    "\n",
    "    # On liste les positions de chaque valeur prédite et si aucune n'est valide, on retourne (-1.0, 0.0) :\n",
    "    valid_mask = (\n",
    "        mask_missing_values\n",
    "        & np.isfinite(true_holed)\n",
    "        & np.isfinite(predicted_holed)\n",
    "    )\n",
    "    if valid_mask.sum() == 0:\n",
    "        return -1.0, 0.0\n",
    "\n",
    "    # On extrait les valeurs valides :\n",
    "    true_values = true_holed[valid_mask]\n",
    "    pred_values = predicted_holed[valid_mask]\n",
    "\n",
    "    # Calcul de la MAE :\n",
    "    diff = np.abs(true_values - pred_values)\n",
    "    mae = float(diff.mean())\n",
    "\n",
    "    # Calcul du R² :\n",
    "    ss_res = float(np.sum((true_values - pred_values) ** 2))\n",
    "    ss_tot = float(np.sum((true_values - true_values.mean()) ** 2))\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n",
    "\n",
    "    return mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31b2b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lit le fichier CSV donné et retourne un DataFrame pandas de ce fichier avec la colonne 'Horodate' en tant qu'index.\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le fichier CSV à lire.\n",
    "    Raises:\n",
    "        ValueError: Si la colonne 'Horodate' est absente du CSV.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame pandas avec les données du CSV, sans la colonne 'Horodate'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if 'Horodate' not in df.columns:\n",
    "        raise ValueError(\"Le fichier CSV doit contenir une colonne 'Horodate'.\")\n",
    "\n",
    "    df.set_index('Horodate', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14419814",
   "metadata": {},
   "source": [
    "### B. Fonction principale d’imputation avec hyperparamètres optimisés\n",
    "\n",
    "On définit la fonction `run_imputation` qui :\n",
    "- Charge le fichier d'entrée (train ou test),\n",
    "- Identifie les colonnes `holed_*`,\n",
    "- Supprime les features constantes,\n",
    "- Lance `impute_holed_columns`,\n",
    "- Sauvegarde les prédictions,\n",
    "- Et, si on est en mode train, calcule MAE et R² avec `get_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imputation(\n",
    "        nb_neighbors: int,\n",
    "        ridge_parameters: dict,\n",
    "        hgbr_parameters: dict,\n",
    "        ridge_proportion: float,\n",
    "        train_mode: bool = False,\n",
    "        input_file: str = None,\n",
    "        output_file: str = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Fonction principale pour exécuter l'imputation des colonnes trouées dans un fichier donné.\n",
    "    Charge les données, effectue l'imputation et sauvegarde les résultats dans un fichier CSV.\n",
    "\n",
    "    Args:\n",
    "        nb_neighbors (int): Nombre de voisins les plus corrélés à utiliser pour l'imputation.\n",
    "        ridge_parameters (dict): Dictionnaire des hyperparamètres pour le modèle Ridge.\n",
    "        hgbr_parameters (dict): Dictionnaire des hyperparamètres pour le modèle HGBR.\n",
    "        ridge_proportion (float): Poids pour la prédiction Ridge dans l'ensemble.\n",
    "        train_mode (bool):\n",
    "            Si vrai, réalise une prédiction sur le dataset de train et renvoie le MAE & R² associée.\n",
    "            Si faux, réalise une prédiction sur le dataset de test.\n",
    "        input_file (str):\n",
    "            Chemin vers le fichier d'entrée contenant les données avec trous.\n",
    "            Par défaut 'data/datasets/x_train.csv' si train_mode == True, sinon 'data/datasets/x_test.csv'.\n",
    "        output_file (str):\n",
    "            Chemin vers le fichier de sortie pour enregistrer les données imputées.\n",
    "            Par défaut 'y_train.csv' si train_mode == True, sinon 'y_test.csv'.\n",
    "    \"\"\"\n",
    "    # Sélection des fichiers d'entrée et de sortie en fonction du mode (train/test) :\n",
    "    if input_file is None:\n",
    "        input_file = 'data/datasets/x_train.csv' if train_mode else 'data/datasets/x_test.csv'\n",
    "    if output_file is None:\n",
    "        output_file = 'y_train.csv' if train_mode else 'y_test.csv'\n",
    "\n",
    "    # Chargement du fichier d'entrée :\n",
    "    print(f\"Chargement du fichier '{input_file}' :\")\n",
    "    df = get_dataframe(input_file)\n",
    "\n",
    "    # Extraction des colonnes à prédire 'holed_*' :\n",
    "    holed_to_predict = [\n",
    "        column for column in df.columns if 'holed_' in column\n",
    "    ]\n",
    "    if not holed_to_predict:\n",
    "        raise ValueError(\"Aucune colonne cible 'holed_' trouvée dans le fichier d'entrée.\")\n",
    "\n",
    "    # Extration des colonnes d'entrainement (complètes et non-constantes) :\n",
    "    potential_predictors = df.columns.difference(holed_to_predict)\n",
    "    stds = df[potential_predictors].std()\n",
    "    complete_columns = stds[stds > 1e-9].index.tolist() # On enlève les colonnes constantes car elles sont (quasiment) inutiles pour Ridge et HGBR.\n",
    "\n",
    "    print(f\"\\t- Colonnes à prédire (holed)\\t = {len(holed_to_predict)}\\n\\t- Colonnes d'entraînement\\t = {len(complete_columns)}\")\n",
    "\n",
    "    # Lancement de l'imputation des données :\n",
    "    print(f\"\\nImputation de {len(holed_to_predict)} colonnes :\")\n",
    "    holed_predicted = impute_holed_columns(df, holed_to_predict, complete_columns, nb_neighbors, ridge_parameters, hgbr_parameters, ridge_proportion)\n",
    "    # Sauvegarde du résultat :\n",
    "    print(f\"\\nEnregistrement des prédictions dans '{output_file}'.\")\n",
    "    holed_predicted.to_csv(output_file, index=False)\n",
    "\n",
    "    # Si on est en mode train (utilise le dataset x_train), on affiche la MAE et le R² associé :\n",
    "    if train_mode:\n",
    "        holed_true = get_dataframe('data/datasets/y_train_true.csv').filter(like='holed_')\n",
    "        holed_nans = get_dataframe('data/datasets/x_train.csv').filter(like='holed_')\n",
    "\n",
    "        mae, r2 = get_metrics(holed_true, holed_predicted.filter(like='holed_'), holed_nans)\n",
    "        print(f\"\\nMétriques [MODE TRAIN] :\\n\\t- MAE\\t = {mae}\\n\\t- R2\\t = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1e378",
   "metadata": {},
   "source": [
    "### C. Exécution complète de l’imputation\n",
    "\n",
    "Il ne nous reste plus qu'à appeler la précédente fonction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = False # Si vrai, réalise une prédiction sur le dataset de train et renvoie le MAE & R² associée. Si faux, réalise une prédiction sur le dataset de test.\n",
    "\n",
    "run_imputation(\n",
    "    nb_neighbors = NB_NEIGHBORS,\n",
    "    ridge_parameters = RIDGE_PARAMETERS,\n",
    "    hgbr_parameters = HGBR_PARAMETERS,\n",
    "    ridge_proportion = RIDGE_PROPORTION,\n",
    "    train_mode = TRAIN_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bf5212",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "### Régression Ridge\n",
    "- Documentation scikit-learn, `sklearn.linear_model.Ridge` (référence API)  \n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html  \n",
    "- Guide utilisateur scikit-learn — *1.1. Modèles linéaires : régression Ridge*  \n",
    "  https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\n",
    "\n",
    "### Histogram-based Gradient Boosting Regressor (HGBR)\n",
    "- Documentation scikit-learn, `sklearn.ensemble.HistGradientBoostingRegressor` (référence API)  \n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html  \n",
    "- Guide utilisateur scikit-learn — *1.11. Méthodes d’ensemble : Histogram-Based Gradient Boosting*  \n",
    "  https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting  \n",
    "- YouTube, *Hist Gradient Boosting Regression*  \n",
    "  https://www.youtube.com/watch?v=V3pXpCnLgqw\n",
    "\n",
    "### Optuna (optimisation d’hyperparamètres)\n",
    "- Site officiel, Optuna : framework d’optimisation d’hyperparamètres  \n",
    "  https://optuna.org/  \n",
    "- Documentation officielle et tutoriels (Read the Docs)  \n",
    "  https://optuna.readthedocs.io/en/stable/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
